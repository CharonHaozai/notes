# (一)线性回归
## 1.模型
线性回归假设输出与各个输入之间是线性关系,下面是一个简单的线性回归模型:
$$
\hat{y} = x_1w_1+x_2w_2+b
$$
### 2.损失函数
索引为i的样本误差:
$$\ell^{(i)}(w_1,w_2,b)=\frac{1}{2}\Big(\hat{y}^{(i)}-y^{(i)}\Big)^2$$
常数$\frac{1}{2}$使对平方项求导之后常数项系数为1.
样本误差的平均衡量模型预测的质量（衡量误差的函数成为损失函数）:

$$\ell(w_1,w_2,b)=\frac1n\sum_{i=1}^n\ell^{(i)}(w_1,w_2,b)=\frac1n\sum_{i=1}^n\frac12\Big(x_1^{(i)}w_1+x_2^{(i)}w_2+b-y^{(i)}\Big)^2$$
### 3.优化算法
小批量随机梯度下降进行优化:
$$\begin{gathered}
w_{1}\leftarrow w_{1}-\frac{\eta}{|\mathcal{B}|}\sum_{i\in\mathcal{B}}\frac{\partial\ell^{(i)}(w_{1},w_{2},b)}{\partial w_{1}}=w_{1}-\frac{\eta}{|\mathcal{B}|}\sum_{i\in\mathcal{B}}x_{1}^{(i)}\left(x_{1}^{(i)}w_{1}+x_{2}^{(i)}w_{2}+b-y^{(i)}\right) \\
w_{2}\leftarrow w_{2}-\frac{\eta}{|\mathcal{B}|}\sum_{i\in\mathcal{B}}\frac{\partial\ell^{(i)}(w_{1},w_{2},b)}{\partial w_{2}}=w_{2}-\frac{\eta}{|\mathcal{B}|}\sum_{i\in\mathcal{B}}x_{2}^{(i)}\left(x_{1}^{(i)}w_{1}+x_{2}^{(i)}w_{2}+b-y^{(i)}\right)\\
b\leftarrow b-\frac\eta{|\mathcal{B}|}\sum_{i\in\mathcal{B}}\frac{\partial\ell^{(i)}(w_{1},w_{2},b)}{\partial b}=b-\frac\eta{|\mathcal{B}|}\sum_{i\in\mathcal{B}}\left(x_{1}^{(i)}w_{1}+x_{2}^{(i)}w_{2}+b-y^{(i)}\right)
\end{gathered}$$
​ $\mathcal{B}$表示每个小批量中的样本数，$\eta$表示学习率，批量大小和学习率的值通常是手动预先指定（称为超参数）.可以由表达式看出,就是不断迭代减去损失函数的对于三个参数的偏导数的平均.
至于为什么使用梯度下降算法而不是直接令导数为0求解,可参看[这篇回答](https://www.zhihu.com/question/20319985):
>不是所有的函数都可以根据导数求出取得0值的点的, 现实的情况可能是:
>1.可以求出导数在每个点的值, 但是直接解方程解不出来, 比如一些简单的神经网络
>2.导数没有解析解, 像一个黑匣子一样, 给定输入值, 可以返回输出值, 但是具体里面是什么情况, 搞不清楚, 工程上似乎有这种情况
>以上两种就不能直接令导数为0求解.
>牛顿迭代和梯度下降法都可以计算极值, 区别在于, 梯度下降法的算法复杂度低一些, 但是迭代次数多一些; 牛顿迭代法计算的更快(初值必须设置的很合理), 但是牛顿迭代法因为有"除法"参与(对矩阵来说就是求逆矩阵), 所以每一步迭代计算量很大. 一般会根据具体的情况取舍.
### 4.矢量表示
广义上,当数据样本为n,表达式变为:
$$\hat{\boldsymbol{y}}=\boldsymbol{X}\boldsymbol{w}+b$$
损失函数变为:
$$\ell(\boldsymbol{\theta})=\frac{1}{2n}(\boldsymbol{\hat{y}}-\boldsymbol{y})^\top(\boldsymbol{\hat{y}}-\boldsymbol{y})$$
迭代步骤变成:
$$\theta\leftarrow\theta-\frac{\eta}{|\mathcal{B}|}\sum_{i\in\mathcal{B}}\nabla_{\boldsymbol{\theta}}\ell^{(i)}(\boldsymbol{\theta})$$
$$\nabla_{\boldsymbol{\theta}}\ell^{(i)}(\boldsymbol{\theta})=\begin{bmatrix}\frac{\partial\ell^{(i)}(w_1,w_2,b)}{\partial w_1}\\\frac{\partial\ell^{(i)}(w_1,w_2,b)}{\partial w_2}\\\frac{\partial\ell^{(i)}(w_1,w_2,b)}{\partial b}\end{bmatrix}=\begin{bmatrix}x_1^{(i)}(x_1^{(i)}w_1+x_2^{(i)}w_2+b-y^{(i)})\\x_2^{(i)}(x_1^{(i)}w_1+x_2^{(i)}w_2+b-y^{(i)})\\x_1^{(i)}w_1+x_2^{(i)}w_2+b-y^{(i)}\end{bmatrix}=\begin{bmatrix}x_1^{(i)}\\x_2^{(i)}\\1\end{bmatrix}(\hat{y}^{(i)}-y^{(i)})$$
### 5.为什么选择均方损失
均方损失函数可以用于线性回归的一个原因是:假设了观测中包含噪声,其中噪声服从正态分布:
$$y=\mathbf{w}^\top\mathbf{x}+b+\epsilon$$其中，$\epsilon\sim\mathcal{N}(0,\sigma^2)$
首先复习一下概率论中的极大似然估计:
似然就是由已经发生的结果来推测产生这个结果的可能环境.
举个栗子,假设进行了n次独立随机测验,其中"状态1"发生了$n_1$次,"状态2"发生了$n_2$次(从经验和直觉出发,状态1发生的概率是$\frac{n_1}{n_1+n_2}$)定义似然函数$L(\theta)=\theta^{n_1}(1-\theta)^{n_2}$,使得似然函数最大,就可以求出$\hat{\theta}=\frac{n_1}{n_1+n_2}$.
在机器学习中使用极大似然估计的算法有朴素贝叶斯、EM算法等.利用极大似然估计建立的损失函数模型，需要进一步借助**梯度下降法**来不断的更新迭代参数，来对参数进行求解。
而在本节中,y的似然:
$$P(y\mid\mathbf{x})=\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{1}{2\sigma^2}(y-\mathbf{w}^\top\mathbf{x}-b)^2\right)$$
求似然函数的最大(由于历史原因这里取最小):
$$-\log P(\mathbf{y}\mid\mathbf{X})=-\log (\prod_{i=1}^np(y^{(i)}|\mathbf{x}^{(i)}))=\sum_{i=1}^n \left(\frac{1}{2}\log(2\pi\sigma^2)+\frac{1}{2\sigma^2}\left(y^{(i)}-\mathbf{w}^\top\mathbf{x}^{(i)}-b\right)^2\right)$$
后一项说明在高斯噪声的假设下最小均方误差等价于对线性模型的极大似然估计.
## (二)线性回归的从零开始实现
### 1.生成数据集
```python
%matplotlib inline
import random
import torch
from d2l import torch as d2l
def synthetic_data(w, b, num_examples):  #@save
    """生成y=Xw+b+噪声"""
    X = torch.normal(0, 1, (num_examples, len(w))) # 正态分布的随机矩阵，num_examples指定样本数量，len(w)指定列数
    y = torch.matmul(X, w) + b  # 矩阵和向量相乘
    y += torch.normal(0, 0.01, y.shape)  # 添加噪声
    return X, y.reshape((-1, 1))

true_w = torch.tensor([2, -3.4])
true_b = 4.2
features, labels = synthetic_data(true_w, true_b, 1000)
```
### 2.读取数据集
以下是一个获取小批量数据的代码:
```python
def data_iter(batch_size, features, labels):
    num_examples = len(features)
    indices = list(range(num_examples))
    # 这些样本是随机读取的，没有特定的顺序
    random.shuffle(indices)
    for i in range(0, num_examples, batch_size):
        batch_indices = torch.tensor(
            indices[i: min(i + batch_size, num_examples)])
        yield features[batch_indices], labels[batch_indices]  # 这是一个生成器，实际上深度学习框架中实现的内置迭代器要比这高效得多

batch_size = 10
for X, y in data_iter(batch_size, features, labels):
    print(X, '\n', y)
    break
# 将数据以10个为一组随机分配，取其中的一组
```
### 3.模型与模型参数
从均值为0、标准差为0.01的正态分布中采样随机数来初始化权重， 并将偏置初始化为0.初始化后后续更新这些参数来拟合数据.
```python
w = torch.normal(0, 0.01, size=(2,1), requires_grad=True)
b = torch.zeros(1, requires_grad=True)
```
定义模型:
```python
def linreg(X, w, b):  #@save
    """线性回归模型"""
    return torch.matmul(X, w) + b
```
定义损失函数:
```python
def squared_loss(y_hat, y):  #@save
    """均方损失"""
    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2
```
定义优化算法(小批量随机梯度下降):
```python
def sgd(params, lr, batch_size):  #@save
    """小批量随机梯度下降"""
    with torch.no_grad():
        for param in params:
            param -= lr * param.grad / batch_size
            param.grad.zero_()  # 在每次参数更新后，我们需要清除旧的梯度，以便于下一次迭代时计算新的梯度
```
### 4.训练
在每次迭代中，读取一小批量训练样本，并通过模型来获得一组预测。 计算完损失后，开始反向传播，存储每个参数的梯度。 最后，调用优化算法`sgd`来更新模型参数。在机器学习中，需要多次遍历整个训练数据集（即多个epoch），在每个迭代周期（epoch）中，使用`data_iter`函数遍历整个数据集， 并将训练数据集中所有样本都使用一次（假设样本数能够被批量大小整除）,`num_epochs`和学习率`lr`都是超参数，分别设为3和0.03(设置超参数需要反复试验调整)。
```python
lr = 0.03
num_epochs = 3
net = linreg
loss = squared_loss

for epoch in range(num_epochs):
    for X, y in data_iter(batch_size, features, labels):
        l = loss(net(X, w, b), y)  # X和y的小批量损失
        # 因为l形状是(batch_size,1)，而不是一个标量。l中的所有元素被加到一起，
        # 并以此计算关于[w,b]的梯度
        l.sum().backward()  # 原地操作计算梯度存储在.grad属性中
        sgd([w, b], lr, batch_size)  # 使用参数的梯度更新参数
    with torch.no_grad():  # 暂时禁用梯度计算
        train_l = loss(net(features, w, b), labels)
        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')
```
## (三)线性回归的简洁实现
利用框架提供的一些API,能够对线性回归进行简单实现.
**首先**还是生成数据集.
**然后**读取数据集使用Pytorch提供的API进行封装:
```python
def load_array(data_arrays, batch_size, is_train=True):  #@save
    """构造一个PyTorch数据迭代器"""
    dataset = data.TensorDataset(*data_arrays) # TensorDataset是PyTorch中用于存储数据和标签的类
    return data.DataLoader(dataset, batch_size, shuffle=is_train)

batch_size = 10
data_iter = load_array((features, labels), batch_size)
```
这里的`is_train`表示是否希望数据迭代器对象在每个迭代周期内打乱数据,与上节的data_iter不同,这里我们使用`iter`构造Python迭代器,并使用`next`从迭代器中获取第一项.
**然后**定义模型和模型参数,使用Pytorch提供的Sequential类使用层来构造模型(其实可以不使用,但后续许多模型是多层的也会用到),使用Linear类来输入全连接层:
```python
# nn是神经网络的缩写
from torch import nn
net = nn.Sequential(nn.Linear(2, 1))
```
损失函数:
```python
loss = nn.MSELoss()
```
优化算法:
```python
trainer = torch.optim.SGD(net.parameters(), lr=0.03)
```
**最后**训练:
```python
num_epochs = 3
for epoch in range(num_epochs):
    for X, y in data_iter:
        l = loss(net(X) ,y)
        trainer.zero_grad()  # 反向传播之前需要将梯度归零因为默认情况下梯度是累加的
        l.backward()
        trainer.step()  # 更新网络参数
    l = loss(net(features), labels)
    print(f'epoch {epoch + 1}, loss {l:f}')
```
## (四)softmax回归
回归可以用于预测多少的问题,也可以用于分类问题