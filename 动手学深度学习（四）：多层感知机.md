# （一）多层感知机
## 1.多层感知机介绍
如图是一张单隐藏层的感知机,当有许多层的时候,最后一层看作线性预测器,这种架构就叫做多层感知机(multilayer perceptron 即MLP).
![单隐藏层的多层感知机](https://zh.d2l.ai/_images/mlp.svg)
再应用上激活函数$\sigma$就能让多层感知机不再是简单的线性模型从而能够表示任何仿射函数.如下所示:
$$
\begin{aligned}
\mathbf{H}&=\sigma(\mathbf{X}\mathbf{W}^{(1)}+\mathbf{b}^{(1)}),
\\\mathbf{O}&=\mathbf{H}\mathbf{W}^{(2)}+\mathbf{b}^{(2)}.
\end{aligned}
$$
## 2.常见激活函数
_激活函数_（activation function）通过计算加权和并加上偏置来确定神经元是否应该被激活， 它们将输入信号转换为输出的可微运算。
### 2.1 ReLU(Rectified linear unit)函数
$\mathrm{ReLU}(x) = max(x,0)$
图像:
![ReLU(Rectified linear unit)函数](https://zh.d2l.ai/_images/output_mlp_76f463_21_0.svg)
### 2.2 sigmoid函数
前面已经提到过该函数:$\mathrm{sigmoid}(x)=\frac{1}{1+\exp(-x)}$
图像:
![sigmoid函数](https://zh.d2l.ai/_images/output_mlp_76f463_48_0.svg)
### 2.3 tanh函数
双曲正切函数:$\tanh(x)=\frac{1-\exp(-2x)}{1+\exp(-2x)}$
图像:
![tanh函数](https://zh.d2l.ai/_images/output_mlp_76f463_78_0.svg)
# (二) 多层感知机的实现
## 1.从零开始实现
完整代码如下:
```python
import torch
from torch import nn
from d2l import torch as d2l

batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)

num_inputs, num_outputs, num_hiddens = 784, 10, 256

W1 = nn.Parameter(torch.randn(
    num_inputs, num_hiddens, requires_grad=True) * 0.01)
b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=True))
W2 = nn.Parameter(torch.randn(
    num_hiddens, num_outputs, requires_grad=True) * 0.01)
b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=True))

params = [W1, b1, W2, b2]

def relu(X):
    a = torch.zeros_like(X)
    return torch.max(X, a)
loss = nn.CrossEntropyLoss(reduction='none')
# 训练
num_epochs, lr = 10, 0.1
updater = torch.optim.SGD(params, lr=lr)  # 随机梯度下降
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)
```
可以看出基本和之前线性神经网络从零开始实现类似,不过多了一层以及激活函数使用了自定义的relu函数.
## 2.简洁实现
完整代码如下:
```python
import torch
from torch import nn
from d2l import torch as d2l

net = nn.Sequential(nn.Flatten(),
                    nn.Linear(784, 256),
                    nn.ReLU(),
                    nn.Linear(256, 10))

def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, std=0.01)
        # nn.init.normal_函数将线性层的权重向量中的每个元素都初始化为一个正态随机值

net.apply(init_weights);
# 训练
batch_size, lr, num_epochs = 256, 0.1, 10
loss = nn.CrossEntropyLoss(reduction='none')
trainer = torch.optim.SGD(net.parameters(), lr=lr)

train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
```
# (三) 模型选择
原文废话比较多,简而言之就是训练的模型会有训练误差和泛化误差.训练误差是在训练集上训练得到的误差,泛化误差是无穷集中误差的期望值.由此我们可以知道模型会有欠拟合和过拟合,通俗的讲欠拟合就是训练误差大,表达能力不足,不能准确预测;而过拟合是指模型在训练数据上表现很好,但在新的、未见过的数据上表现不佳.
过拟合有以下一些影响因素:
- 可调整参数的数量。当可调整参数的数量（有时称为自由度）很大时，模型往往更容易过拟合。
- 参数采用的值。当权重的取值范围较大时，模型可能更容易过拟合。
- 训练样本的数量。即使模型很简单，也很容易过拟合只包含一两个样本的数据集。而过拟合一个有数百万个样本的数据集则需要一个极其灵活的模型。
由此出现了**验证集**:将我们的数据分成三份， 除了训练和测试数据集之外，还增加一个验证数据集（validation dataset）， 也叫验证集（validation set）.但现实是验证数据和测试数据之间的边界模糊得令人担忧...训练数据稀缺时,使用**K折交叉验证**:原始训练数据被分成个不重叠的子集。 然后执行次模型训练和验证，每次在个子集上进行训练， 并在剩余的一个子集（在该轮中没有用于训练的子集）上进行验证。 最后，通过对次实验的结果取平均来估计训练和验证误差.
# (四) 正则化模型技术
正则化（Regularization）是机器学习中用来防止模型过拟合的一种技术。
## 1.权重衰减($L_2$正则化)
之前有提到过范数,范数能够度量向量大小.因为在很多模型中，尤其是线性模型，参数的大小可以反映模型的复杂度。较大的参数值意味着模型对输入数据的变化更敏感，这可能导致模型在训练数据上过度拟合，而在未见过的数据上表现不佳.所以最小化$L_2$范数，可以防止任何单个权重变得过大，这有助于保持模型的稳定性.
而$L_2$正则化也就是权重衰减就是在损失函数上加上一个范数作为惩罚项,将原来的训练目标最小化训练标签上的预测损失,调整为最小化预测损失和惩罚项之和.现在,如果我们的权重向量增长的太大,我们的学习算法可能会更集中于最小化权重范数.
实际上还有$L_1$正则化,那为什么选择$L_2$范数呢.一个原因是它对权重向量的大分量施加了巨大的惩罚.这使得我们的学习算法偏向于在大量特征上均匀分布权重的模型.相比之下,$L_1$惩罚会导致模型将权重集中在一小部分特征上,而将其他权重清除为零.这称为特征选择（feature selection）.
于是平衡后的新的额外惩罚的损失变为:
$$
L(\mathbf{w},b)+\frac{\lambda}{2}\|\mathbf{w}\|^2
$$
$\lambda$称为正则化系数,较小的$\lambda$值对应较少约束的$\mathbf{w}$,而较大的$\lambda$值对$\mathbf{w}$的约束更大.$\lambda$除以2仍然是因为求导可以抵消平方项的$\frac{1}{2}$,选择平方范数而不是标准范数也是为了方便计算.
于是小批量随机梯度下降变为:
$$
\begin{aligned}
\mathbf{w}\leftarrow(1-\eta\lambda)\mathbf{w}-\frac{\eta}{|\mathcal{B}|}\sum_{i\in\mathcal{B}}\mathbf{x}^{(i)}\left(\mathbf{w}^{\top}\mathbf{x}^{(i)}+b-y^{(i)}\right)
\end{aligned}
$$
所以在更新$\mathbf{w}$的过程中同时也在试图将$\mathbf{w}$的大小缩到零.
### 举例演示
```python
%matplotlib inline
import torch
from torch import nn
from d2l import torch as d2l

# 初始化模型参数
def init_params():
    w = torch.normal(0, 1, size=(num_inputs, 1), requires_grad=True)
    b = torch.zeros(1, requires_grad=True)
    return [w, b]
# 定义范数惩罚
def l2_penalty(w):
    return torch.sum(w.pow(2)) / 2
def train(lambd):
    w, b = init_params()
    net, loss = lambda X: d2l.linreg(X, w, b), d2l.squared_loss
    num_epochs, lr = 100, 0.003
    animator = d2l.Animator(xlabel='epochs', ylabel='loss', yscale='log',
                            xlim=[5, num_epochs], legend=['train', 'test'])
    for epoch in range(num_epochs):
        for X, y in train_iter:
            # 增加了L2范数惩罚项，
            # 广播机制使l2_penalty(w)成为一个长度为batch_size的向量
            l = loss(net(X), y) + lambd * l2_penalty(w)
            l.sum().backward()
            d2l.sgd([w, b], lr, batch_size)
        if (epoch + 1) % 5 == 0:
            animator.add(epoch + 1, (d2l.evaluate_loss(net, train_iter, loss),
                                     d2l.evaluate_loss(net, test_iter, loss)))
    print('w的L2范数是：', torch.norm(w).item())
```
当$\lambda$为0的时候也就是没有引入惩罚的时候得到:
![](https://zh.d2l.ai/_images/output_weight-decay_ec9cc0_81_1.svg)
当设置$\lambda$为3后得到:
![](https://zh.d2l.ai/_images/output_weight-decay_ec9cc0_96_1.svg)
通过比较就可以发现,引入$\lambda$后,虽然最后训练数据集的误差到后面降不下去了,但是测试数据集中的误差得到了下降.
**简洁实现**
```python
# weight decay作为参数输入
def train_concise(wd):
    net = nn.Sequential(nn.Linear(num_inputs, 1))
    for param in net.parameters():
        param.data.normal_()
    loss = nn.MSELoss(reduction='none')
    num_epochs, lr = 100, 0.003
    # 偏置参数没有衰减
    trainer = torch.optim.SGD([
        {"params":net[0].weight,'weight_decay': wd}, 
        {"params":net[0].bias}] # 只为权重设置了weight_decay，所以偏置参数b(bias)不会衰减
        , lr=lr)
    animator = d2l.Animator(xlabel='epochs', ylabel='loss', yscale='log',
                            xlim=[5, num_epochs], legend=['train', 'test'])
    for epoch in range(num_epochs):
        for X, y in train_iter:
            trainer.zero_grad()
            l = loss(net(X), y)
            l.mean().backward()
            trainer.step()
        if (epoch + 1) % 5 == 0:
            animator.add(epoch + 1,
                         (d2l.evaluate_loss(net, train_iter, loss),
                          d2l.evaluate_loss(net, test_iter, loss)))
    print('w的L2范数：', net[0].weight.norm().item())
```
## 2.暂退法(Dropout)
摘自原文:
> 深度网络的泛化性质令人费解，而这种泛化性质的数学基础仍然是悬而未决的研究问题。 我们鼓
> 励喜好研究理论的读者更深入地研究这个主题。 本节，我们将着重对实际工具的探究，这些工具
> 倾向于改进深层网络的泛化性。

说明还没有完美解决过拟合的问题...
我们期待“好”的预测模型能在未知的数据上有很好的表现： 经典泛化理论认为，为了缩小训练和测试性能之间的差距，应该以简单的模型为目标。权重衰减中参数的范数也代表了一种有用的简单性度量。简单性的另一个角度是平滑性，即函数不应该对其输入的微小变化敏感。如果平滑那添加一些随机噪声应该是基本无影响的.所以[Srivastava _et al._, 2014](https://zh.d2l.ai/chapter_references/zreferences.html#id155 "Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. (2014). Dropout: a simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1), 1929–1958.")等人就想出了在网络中注入噪声.**暂退法**就是在训练过程中随机“丢弃”（即暂时移除）网络中的一些神经元（及其连接），来减少神经元之间复杂的共适应关系，增强模型的泛化能力。
有趣的是暂退法的原始论文提到的是有性生殖的类比,这说明深度学习与生物学有极其重要的联系,更别提今年的物理和化学诺奖都落入深度学习之手...
### 举例演示
```python
import torch
from torch import nn
from d2l import torch as d2l

def dropout_layer(X, dropout):
    assert 0 <= dropout <= 1
    # 在本情况中，所有元素都被丢弃
    if dropout == 1:
        return torch.zeros_like(X)
    # 在本情况中，所有元素都被保留
    if dropout == 0:
        return X
    mask = (torch.rand(X.shape) > dropout).float() 
    # 使用torch.rand(X.shape)生成一个与X形状相同的随机数张量，其中的每个元素都是从0到1的均匀分布中随机抽取的,通过比较这个随机数张量和dropout值，生成一个布尔值的掩码（mask）
    return mask * X / (1.0 - dropout) # 使期望值不变

num_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256
# 定义一个具有两个隐藏层的多层感知机，每个隐藏层包含256个单元

dropout1, dropout2 = 0.2, 0.5

class Net(nn.Module):
    def __init__(self, num_inputs, num_outputs, num_hiddens1, num_hiddens2,
                 is_training = True):
        super(Net, self).__init__() # 父类构造函数
        self.num_inputs = num_inputs
        self.training = is_training
        self.lin1 = nn.Linear(num_inputs, num_hiddens1)
        self.lin2 = nn.Linear(num_hiddens1, num_hiddens2)
        self.lin3 = nn.Linear(num_hiddens2, num_outputs)
        self.relu = nn.ReLU()

    def forward(self, X):
        H1 = self.relu(self.lin1(X.reshape((-1, self.num_inputs))))
        # 只有在训练模型时才使用dropout
        if self.training == True:
            # 在第一个全连接层之后添加一个dropout层
            H1 = dropout_layer(H1, dropout1)
        H2 = self.relu(self.lin2(H1))
        if self.training == True:
            # 在第二个全连接层之后添加一个dropout层
            H2 = dropout_layer(H2, dropout2)
        out = self.lin3(H2)
        return out

net = Net(num_inputs, num_outputs, num_hiddens1, num_hiddens2)

# 训练
num_epochs, lr, batch_size = 10, 0.5, 256
loss = nn.CrossEntropyLoss(reduction='none')
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
trainer = torch.optim.SGD(net.parameters(), lr=lr)
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
```
**简洁实现**
```python
net = nn.Sequential(nn.Flatten(),
        nn.Linear(784, 256),
        nn.ReLU(),
        # 在第一个全连接层之后添加一个dropout层
        nn.Dropout(dropout1),
        nn.Linear(256, 256),
        nn.ReLU(),
        # 在第二个全连接层之后添加一个dropout层
        nn.Dropout(dropout2),
        nn.Linear(256, 10))

def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, std=0.01)

net.apply(init_weights);

# 训练
trainer = torch.optim.SGD(net.parameters(), lr=lr)
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
```
# (五) 前向传播、反向传播和计算图
## 1.前向传播
_前向传播_（forward propagation或forward pass） 指的是：按顺序（从输入层到输出层）计算和存储神经网络中每层的结果。
举个栗子(对于有两层隐藏层的多层感知机):
$$
\begin{aligned}
\mathbf{z}&=\mathbf{W}^{(1)}\mathbf{x}\\
\mathbf h&=\phi(\mathbf z) \\
\mathbf o&=\mathbf W^{(2)}\mathbf h \\
L&=l(\mathbf{o},y)\\
正则化项:
s&=\frac{\lambda}{2}\Big(\|\mathbf{W}^{(1)}\|_F^2+\|\mathbf{W}^{(2)}\|_F^2\Big)\\
正则化损失(目标函数):
J&=L+s
\end{aligned}
$$
前向传播计算图:
![前向传播图](https://zh.d2l.ai/_images/forward.svg)
## 2. 反向传播

